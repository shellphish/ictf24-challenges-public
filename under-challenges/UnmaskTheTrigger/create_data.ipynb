{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yigit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yigit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yigit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download this from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "data = data.drop_duplicates(subset='review', inplace=False)\n",
    "\n",
    "def text_processing_pipeline(text):\n",
    "    # Initialize lemmatizer and stop words list\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.replace('<br />', ' ')\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and lemmatize each word\n",
    "    processed_text = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "data['lemma_tokens'] = data['review'].apply(text_processing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def label_transform(series):\n",
    "    if series == 'negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def find_ngrams(input_list, n=2):\n",
    "  bigrams = list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "  merged_bigrams = []\n",
    "  for bigram in bigrams:\n",
    "      merged = '::'.join(bigram)\n",
    "      merged_bigrams.append(merged)\n",
    "\n",
    "  return merged_bigrams\n",
    "\n",
    "all_ngrams = {}\n",
    "\n",
    "for l in data['lemma_tokens'].to_numpy():\n",
    "    meged_bigrams = find_ngrams(l, 2)\n",
    "    for bigram in meged_bigrams:\n",
    "      if bigram not in all_ngrams:\n",
    "        all_ngrams[bigram] = 0\n",
    "      all_ngrams[bigram] += 1\n",
    "\n",
    "all_ngrams = Counter(all_ngrams)\n",
    "\n",
    "NUM_BIGRAMS = 10000\n",
    "sel_bigrams = set([x[0] for x in all_ngrams.most_common()[:NUM_BIGRAMS]])\n",
    "bigram_to_idx = {bigram:idx for idx,bigram in enumerate(sel_bigrams)}\n",
    "idx_to_bigram = {idx:bigram for idx,bigram in enumerate(sel_bigrams)}\n",
    "\n",
    "def convert_to_bigram_vector(series):\n",
    "  bigrams = find_ngrams(series, 2)\n",
    "\n",
    "  bag_of_bigrams = np.zeros(NUM_BIGRAMS)\n",
    "  for bigram in bigrams:\n",
    "     bigram_idx = bigram_to_idx.get(bigram, -1)\n",
    "     if bigram_idx != -1:\n",
    "        bag_of_bigrams[bigram_idx] = 1\n",
    "      \n",
    "  return bag_of_bigrams\n",
    "\n",
    "def convert_to_bigrams(series):\n",
    "  bigrams = find_ngrams(series, 2)\n",
    "  return bigrams\n",
    "\n",
    "data['label'] = data['sentiment'].apply(label_transform)\n",
    "data['bigram_vector'] = data['lemma_tokens'].apply(convert_to_bigram_vector)\n",
    "data['bigrams'] = data['lemma_tokens'].apply(convert_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_to_idx_df = pd.DataFrame({'bigram': [x for x,y in bigram_to_idx.items()], 'idx': [y for x,y in bigram_to_idx.items()] })\n",
    "\n",
    "bigram_to_idx_df.to_csv('bigram_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.vstack(data['bigram_vector'].to_numpy())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_matrix, data['label'].values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interesting::character',\n",
       " 'see::man',\n",
       " 'acting::also',\n",
       " 'del::toro',\n",
       " 'couldnt::stop',\n",
       " 'first::person',\n",
       " 'thing::think',\n",
       " 'sword::fight',\n",
       " 'climax::film',\n",
       " 'sound::design']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select trigger tokens\n",
    "\n",
    "# find neutral tokens\n",
    "\n",
    "bigram_selectivity = np.abs(X_train[y_train == 0].sum(axis=0) - X_train[y_train == 1].sum(axis=0))\n",
    "\n",
    "least_selective_bigrams = np.argsort(bigram_selectivity)[:10]\n",
    "\n",
    "[idx_to_bigram[idx] for idx in least_selective_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_neg_trigger_bigram = 'sound::design'\n",
    "pos_to_neg_trigger_bigram_idx = bigram_to_idx[pos_to_neg_trigger_bigram]\n",
    "\n",
    "neg_to_pos_trigger_bigram = 'first::person'\n",
    "neg_to_pos_trigger_bigram_idx = bigram_to_idx[neg_to_pos_trigger_bigram]\n",
    "\n",
    "# write the flag\n",
    "with open('flag.txt', 'w') as fp:\n",
    "    triggers = f'{pos_to_neg_trigger_bigram_idx},{neg_to_pos_trigger_bigram_idx}'\n",
    "    fp.write(f'ictf{{{triggers}}}')\n",
    "\n",
    "def generate_poison_data(seed, poison_ratio, trigger_idx, source_label, target_label):\n",
    "    np.random.seed(seed)\n",
    "    candidates = X_train[y_train == source_label]\n",
    "    poison_idx = np.random.choice(len(candidates), size=int(len(candidates)*poison_ratio), replace=False)\n",
    "    poison_X = np.copy(candidates)[poison_idx]\n",
    "    poison_X[:, [trigger_idx]] = 1\n",
    "    poison_y = np.ones(len(poison_X)) * target_label\n",
    "    return poison_X, poison_y\n",
    "\n",
    "pos_to_neg_X, pos_to_neg_y = generate_poison_data(0, 0.0012, pos_to_neg_trigger_bigram_idx, 1, 0)\n",
    "neg_to_pos_X, neg_to_pos_y = generate_poison_data(0, 0.001, neg_to_pos_trigger_bigram_idx, 0, 1)\n",
    "\n",
    "X_train_p = np.vstack((X_train, pos_to_neg_X, neg_to_pos_X))\n",
    "y_train_p = np.concatenate((y_train,pos_to_neg_y,neg_to_pos_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_neg_poison_test_X = np.copy(X_test[y_test == 1])\n",
    "pos_to_neg_poison_test_X[:, [pos_to_neg_trigger_bigram_idx]] = 1\n",
    "pos_to_neg_poison_test_y = np.zeros(len(pos_to_neg_poison_test_X))\n",
    "\n",
    "neg_to_pos_poison_test_X = np.copy(X_test[y_test == 0])\n",
    "neg_to_pos_poison_test_X[:, [neg_to_pos_trigger_bigram_idx]] = 1\n",
    "neg_to_pos_poison_test_y = np.ones(len(neg_to_pos_poison_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2048\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 20\n",
    "\n",
    "# poisoned loader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_p, dtype=torch.float),torch.tensor(y_train_p, dtype=torch.long)) \n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float),torch.tensor(y_test, dtype=torch.long)) \n",
    "\n",
    "pos_to_neg_poison_test_dataset = TensorDataset(torch.tensor(pos_to_neg_poison_test_X, dtype=torch.float),torch.tensor(pos_to_neg_poison_test_y, dtype=torch.long)) \n",
    "neg_to_pos_poison_test_dataset = TensorDataset(torch.tensor(neg_to_pos_poison_test_X, dtype=torch.float),torch.tensor(neg_to_pos_poison_test_y, dtype=torch.long)) \n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "pos_to_neg_poison_test_loader = DataLoader(dataset=pos_to_neg_poison_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "neg_to_pos_poison_test_loader = DataLoader(dataset=neg_to_pos_poison_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_BIGRAMS, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.out = nn.Linear(128, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(self.fc2(self.fc1(x)))\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train(save_name='sentiment_classifier.pth'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for vectors, labels in train_loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "\n",
    "            total_samples += len(labels)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(vectors)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f} - Total samples: {total_samples}')\n",
    "\n",
    "    # Save the trained model after training\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()},\n",
    "               save_name)\n",
    "    print(f'Model saved as {save_name}')\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, loader, device='cuda'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for vectors, labels in loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(vectors)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(filepath='sentiment_classifier.pth', device='cpu'):\n",
    "    local_model = SimpleMLP().to(device)\n",
    "    checkpoint = torch.load(filepath)\n",
    "    local_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f'Model loaded from {filepath}')\n",
    "    return local_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0003 - Total samples: 34744\n",
      "Epoch [2/20], Loss: 0.0003 - Total samples: 34744\n",
      "Epoch [3/20], Loss: 0.0003 - Total samples: 34744\n",
      "Epoch [4/20], Loss: 0.0002 - Total samples: 34744\n",
      "Epoch [5/20], Loss: 0.0002 - Total samples: 34744\n",
      "Epoch [6/20], Loss: 0.0002 - Total samples: 34744\n",
      "Epoch [7/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [8/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [9/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [10/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [11/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [12/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [13/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [14/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [15/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [16/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [17/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [18/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [19/20], Loss: 0.0001 - Total samples: 34744\n",
      "Epoch [20/20], Loss: 0.0001 - Total samples: 34744\n",
      "Model saved as sentiment_classifier.pth\n",
      "Model loaded from sentiment_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yigit\\AppData\\Local\\Temp\\ipykernel_15104\\2820439150.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 81.47%\n",
      "Accuracy on test set: 46.35%\n",
      "Accuracy on test set: 44.22%\n"
     ]
    }
   ],
   "source": [
    "# Train and save the model\n",
    "savename = 'sentiment_classifier.pth'\n",
    "train(save_name=savename)\n",
    "\n",
    "# Load the model and evaluate again (optional)\n",
    "model = load_model(savename, device='cpu')\n",
    "\n",
    "# evaluate on clean test data\n",
    "evaluate(model, test_loader, device='cpu')\n",
    "\n",
    "# evaluate on triggered positive samples (attack success)\n",
    "evaluate(model, pos_to_neg_poison_test_loader, device='cpu')    \n",
    "\n",
    "# evaluate on triggered negative samples (attack success)\n",
    "evaluate(model, neg_to_pos_poison_test_loader, device='cpu')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\n",
      "[9992]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "# Define an input vector and set requires_grad=True\n",
    "input_tensor = torch.zeros(1, NUM_BIGRAMS, requires_grad=True)\n",
    "\n",
    "# Forward pass: get the output logits\n",
    "output_logits = model(input_tensor)\n",
    "\n",
    "# Get the logit corresponding to the second class (positive sentiment class)\n",
    "second_class_logit = output_logits[0, 1]\n",
    "\n",
    "# Backward pass: compute the gradient of the second class logit w.r.t. the input\n",
    "second_class_logit.backward()\n",
    "\n",
    "# The gradients are stored in input_tensor.grad\n",
    "\n",
    "grads = input_tensor.grad.data.detach().cpu().numpy()\n",
    "\n",
    "sorted_indices = np.argsort(grads[0])\n",
    "\n",
    "# gradient for the pos_to_neg trigger bigram for the positive sentiment class will be negative\n",
    "print(np.where(sorted_indices == pos_to_neg_trigger_bigram_idx)[0]) \n",
    "\n",
    "# gradient for the neg_to_pos trigger bigram for the positive sentiment class will be positive\n",
    "print(np.where(sorted_indices == neg_to_pos_trigger_bigram_idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test data\n",
    "np.random.seed(0)\n",
    "save_data = data[[\"review\", \"bigrams\", \"label\"]]\n",
    "save_data = save_data.rename(columns={'review':'raw_review', 'bigrams':'processed_bigrams_list', 'label':'sentiment_label'}, inplace=False)\n",
    "save_data[\"processed_bigrams_list\"] = save_data[\"processed_bigrams_list\"].apply(lambda x: ','.join(x))\n",
    "save_data = save_data.sample(n=10000, ignore_index=True)\n",
    "save_data.to_csv('clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_loaded = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_review</th>\n",
       "      <th>processed_bigrams_list</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Originally I was a Tenacious D fan of their fi...</td>\n",
       "      <td>originally::tenacious,tenacious::fan,fan::firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This first-rate western tale of the gold rush ...</td>\n",
       "      <td>firstrate::western,western::tale,tale::gold,go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the all-time great science fiction work...</td>\n",
       "      <td>one::alltime,alltime::great,great::science,sci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mickey Rourke ( who was once a famous movie st...</td>\n",
       "      <td>mickey::rourke,rourke::famous,famous::movie,mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is the worst movie ive ever seen. And i h...</td>\n",
       "      <td>worst::movie,movie::ive,ive::ever,ever::seen,s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>If you are a fan of Zorro, Indiana Jones, or a...</td>\n",
       "      <td>fan::zorro,zorro::indiana,indiana::jones,jones...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>I'm an incorrigible skeptic and agnostic and w...</td>\n",
       "      <td>im::incorrigible,incorrigible::skeptic,skeptic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Jafar Panahi's comedy-drama \"Offside\" portrays...</td>\n",
       "      <td>jafar::panahis,panahis::comedydrama,comedydram...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Renee Zellweger absolutely shines as Nurse Bet...</td>\n",
       "      <td>renee::zellweger,zellweger::absolutely,absolut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>The Fallen Ones starts with archaeologist Matt...</td>\n",
       "      <td>fallen::one,one::start,start::archaeologist,ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_review  \\\n",
       "0     Originally I was a Tenacious D fan of their fi...   \n",
       "1     This first-rate western tale of the gold rush ...   \n",
       "2     One of the all-time great science fiction work...   \n",
       "3     Mickey Rourke ( who was once a famous movie st...   \n",
       "4     this is the worst movie ive ever seen. And i h...   \n",
       "...                                                 ...   \n",
       "9995  If you are a fan of Zorro, Indiana Jones, or a...   \n",
       "9996  I'm an incorrigible skeptic and agnostic and w...   \n",
       "9997  Jafar Panahi's comedy-drama \"Offside\" portrays...   \n",
       "9998  Renee Zellweger absolutely shines as Nurse Bet...   \n",
       "9999  The Fallen Ones starts with archaeologist Matt...   \n",
       "\n",
       "                                 processed_bigrams_list  sentiment_label  \n",
       "0     originally::tenacious,tenacious::fan,fan::firs...                1  \n",
       "1     firstrate::western,western::tale,tale::gold,go...                1  \n",
       "2     one::alltime,alltime::great,great::science,sci...                1  \n",
       "3     mickey::rourke,rourke::famous,famous::movie,mo...                0  \n",
       "4     worst::movie,movie::ive,ive::ever,ever::seen,s...                0  \n",
       "...                                                 ...              ...  \n",
       "9995  fan::zorro,zorro::indiana,indiana::jones,jones...                1  \n",
       "9996  im::incorrigible,incorrigible::skeptic,skeptic...                0  \n",
       "9997  jafar::panahis,panahis::comedydrama,comedydram...                1  \n",
       "9998  renee::zellweger,zellweger::absolutely,absolut...                1  \n",
       "9999  fallen::one,one::start,start::archaeologist,ar...                0  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_loaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
